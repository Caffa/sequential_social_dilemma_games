[flags]
# Experiment flags
experiment_docstring = Name of the experiment to run.
experiment = curiosity

env_docstring = Name of the environment to use. Can be cleanup or harvest.
env = harvest
num_agents_docstring = Number of agent policies.
num_agents = 5

tune_docstring = Set to True to do hyperparameter tuning.
tune = False
resume_docstring = Set to true to resume a previously stopped experiment.
resume = False

# Hardware flags

num_cpus_docstring = Number of available CPUs.
num_cpus = 6
num_gpus_docstring = Number of available GPUs.
num_gpus = 0

use_gpus_for_workers_docstring = Set to true to run workers on GPUs rather than CPUs.
use_gpus_for_workers = False
use_gpu_for_driver_docstring = Set to true to run driver on GPU rather than CPU.
use_gpu_for_driver = False
num_workers_per_device_docstring = Number of workers to place on a single device (CPU or GPU).
num_workers_per_device = 1

# Optional amount of memory (in bytes) to start the ray object store with.
object_store_memory=3e9
# Optional max amount of memory (in bytes) to allow each redis shard to use.
redis_max_memory=

[parameters_cleanup]
entropy_coeff = -.00176
lr_init = 0.00126
lr_final = 0.000012
# Add arbitrary amount of entropy tuning parameters - must start with entropy_tune, is processed in order
entropy_tune_0 = 0
entropy_tune_1 = -1e-1

[parameters_harvest]
entropy_coeff = -.000687
lr_init = 0.00136
lr_final = 0.000028
# Add arbitrary amount of entropy tuning parameters - must start with entropy_tune, is processed in order
entropy_tune_0 = 0
entropy_tune_1 = -1e-1
entropy_tune_2 = -1e-2

# Hyperparameters for the Model of Other Agents (MOA) experiment
[parameters_cleanup_a3c_baseline]
entropy_coeff = -.00176
lr_init = .00126
lr_final = .000012
[parameters_cleanup_moa_baseline]
entropy_coeff = -.00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = 0
moa_loss_weight = 1.312
train_moa_only_when_visible = False
[parameters_cleanup_influence_moa]
entropy_coeff = -.00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = .620
moa_loss_weight = 15.007
curriculum_c = 40
policy_comparison = kl
train_moa_only_when_visible = True
[parameters_harvest_a3c_baseline]
entropy_coeff = -.000687
lr_init = .00136
lr_final = .000028
[parameters_harvest_moa_baseline]
entropy_coeff = -.00495
lr_init = .00206
lr_final = .000022
influence_weight_beta = 0
moa_loss_weight = 1.711
train_moa_only_when_visible = False
[parameters_harvest_influence_moa]
entropy_coeff = -.00223
lr_init = .00120
lr_final = .000044
influence_weight_beta = 2.521
moa_loss_weight = 10.911
curriculum_c = 226
policy_comparison = kl
train_moa_only_when_visible = True
[parameters_switch_a3c_baseline]
entropy_coeff = -.000687
lr_init = .00136
lr_final = .000028
[parameters_switch_curiosity]
entropy_coeff = -.001
lr_init = .01
lr_final = .01

[network]
# Optional Redis address. If this address is not provided, a local Redis server is automatically started.
# Leave empty if not used.
redis_address =
# Optional server address to sync training results to.
# Leave empty if not used.
upload_dir =

[paths]
# Path where ray_results are stored
ray_results_path = ~/ray_results
# Path where plotted ray_results are stored
plot_path = ~/ray_results_plotted
# Path where videos are stored
video_path = ~/ray_results_videos
