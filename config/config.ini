[flags]
# Experiment flags

env_docstring = Name of the environment to use. Can be cleanup or harvest.
env = cleanup
num_agents_docstring = Number of agent policies.
num_agents = 5

tune_docstring = Set to True to do hyperparameter tuning.
tune = False
debug_docstring = Set to true to run in a debugging / testing mode with less memory.
debug = False
resume_docstring = Set to true to resume a previously stopped experiment.
resume = False

# Hardware flags

num_cpus_docstring = Number of available CPUs.
num_cpus = 2
num_gpus_docstring = Number of available GPUs.
num_gpus = 0

use_gpus_for_workers_docstring = Set to true to run workers on GPUs rather than CPUs.
use_gpus_for_workers = False
use_gpu_for_driver_docstring = Set to true to run driver on GPU rather than CPU.
use_gpu_for_driver = False
num_workers_per_device_docstring = Number of workers to place on a single device (CPU or GPU).
num_workers_per_device = 1

# Optional amount of memory (in bytes) to start the ray object store with.
object_store_memory=
# Optional max amount of memory (in bytes) to allow each redis shard to use.
redis_max_memory=

[parameters_cleanup]
entropy_coeff = .00176
lr_init = 0.00126
lr_final = 0.000012
# Add arbitrary amount of entropy tuning parameters - must start with entropy_tune, is processed in order
entropy_tune_0 = 0
entropy_tune_1 = -1e-1

[parameters_harvest]
entropy_coeff = .000687
lr_init = 0.00136
lr_final = 0.000028
# Add arbitrary amount of entropy tuning parameters - must start with entropy_tune, is processed in order
entropy_tune_0 = 0
entropy_tune_1 = -1e-1
entropy_tune_2 = -1e-2

# Experiment 1
# Parameters for the basic influence experiment
[parameters_cleanup_exp1_a3c_baseline]
entropy_coeff = .00176
lr_init = .00126
lr_final = .000012
[parameters_cleanup_exp1_visible_actions_baseline]
entropy_coeff = .00176
lr_init = .00126
lr_final = .000012
number_of_influencers = 3
influence_weight_beta = 0
[parameters_cleanup_exp1_influence]
entropy_coeff = .000248
lr_init = .00107
lr_final = .000042
number_of_influencers = 1
influence_weight_beta = .146
curriculum_c = 140
policy_comparison = jsd
influencee_reward = 1
[parameters_harvest_exp1_a3c_baseline]
entropy_coeff = .000687
lr_init = .00136
lr_final = .000028
[parameters_harvest_exp1_visible_actions_baseline]
entropy_coeff = .00184
lr_init = .00215
lr_final = .000013
number_of_influencers = 3
influence_weight_beta = 0
[parameters_harvest_exp1_influence]
entropy_coeff = .00025
lr_init = .00107
lr_final = .000042
number_of_influencers = 3
influence_weight_beta = .224
curriculum_c = 140
policy_comparison = pmi
influencee_reward = 0

# Experiment 2
# Parameters for the communication experiment
[parameters_cleanup_exp2_a3c_baseline]
entropy_coeff = .00176
lr_init = .00126
lr_final = .000012
[parameters_cleanup_exp2_comm_baseline]
entropy_coeff = .000249
lr_init = .00223
lr_final = .000022
influence_weight_beta = 0
[parameters_cleanup_exp2_influence_comm]
entropy_coeff = .00305
lr_init = .00249
lr_final = .0000127
influence_weight_beta = 2.752
extrinsic_weight_alpha = 0
curriculum_c = 1
policy_comparison = kl
comm_entropy_reg = .000789
comm_loss_weight = .0758
symbol_vocab_size = 9
comm_embedding = 32
[parameters_harvest_exp2_a3c_baseline]
entropy_coeff = .000687
lr_init = .00136
lr_final = .000028
[parameters_harvest_exp2_comm_baseline]
entropy_coeff = .000174
lr_init = .00137
lr_final = .0000127
influence_weight_beta = 0
[parameters_harvest_exp2_influence_comm]
entropy_coeff = .00220
lr_init = .000413
lr_final = .000049
influence_weight_beta = 4.825
extrinsic_weight_alpha = 1.0
curriculum_c = 8
policy_comparison = kl
comm_entropy_reg = .00208
comm_loss_weight = .0709
symbol_vocab_size = 7
comm_embedding = 16

# Experiment 3
# Parameters for the Model of Other Agents (MOA) experiment
[parameters_cleanup_exp3_a3c_baseline]
entropy_coeff = .00176
lr_init = .00126
lr_final = .000012
[parameters_cleanup_exp3_moa_baseline]
entropy_coeff = .00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = 0
moa_loss_weight = 1.312
train_moa_only_when_visible = False
[parameters_cleanup_exp3_influence_moa]
entropy_coeff = .00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = .620
moa_loss_weight = 15.007
curriculum_c = 40
policy_comparison = kl
train_moa_only_when_visible = True
[parameters_harvest_exp3_a3c_baseline]
entropy_coeff = .000687
lr_init = .00136
lr_final = .000028
[parameters_harvest_exp3_moa_baseline]
entropy_coeff = .00495
lr_init = .00206
lr_final = .000022
influence_weight_beta = 0
moa_loss_weight = 1.711
train_moa_only_when_visible = False
[parameters_harvest_exp3_influence_moa]
entropy_coeff = .00223
lr_init = .00120
lr_final = .000044
influence_weight_beta = 2.521
moa_loss_weight = 10.911
curriculum_c = 226
policy_comparison = kl
train_moa_only_when_visible = True


[network]
# Optional Redis address. If this address is not provided, a local Redis server is automatically started.
# Leave empty if not used.
redis_address =
# Optional server address to sync training results to.
# Leave empty if not used.
upload_dir =
