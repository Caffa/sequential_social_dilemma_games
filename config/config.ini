[flags]
# Experiment flags
experiment_docstring = Name of the experiment to run.
experiment = curiosity

env_docstring = Name of the environment to use. Can be cleanup or harvest.
env = switch
num_agents_docstring = Number of agent policies.
num_agents = 1

tune_docstring = Set to True to do hyperparameter tuning.
tune = True
resume_docstring = Set to true to resume a previously stopped experiment.
resume = False
local_mode_docstring = Set to True to force ray into a single thread, allowing for debugging/breakpoints
local_mode = False

stop_at_timesteps_total_docstring = Stopping criterion for experiments. If this amount of timesteps is reached before any other stopping criterion is reached. the experiment ends.
stop_at_timesteps_total = 5e5
stop_at_episode_reward_min_docstring =  Stopping criterion for experiments. When this is the minimal reward reached in a single episode, the experiment is ends.
stop_at_episode_reward_min = 100

# Amount of repeated experiments
num_samples = 1

# Batch sizes during learning
sample_batch_size = 1000
train_batch_size = 30000


# Hardware flags

num_cpus_docstring = Number of available CPUs.
num_cpus = 1
num_gpus_docstring = Number of available GPUs.
num_gpus = 1

use_gpus_for_workers_docstring = Set to true to run workers on GPUs rather than CPUs.
use_gpus_for_workers = False
use_gpu_for_driver_docstring = Set to true to run driver on GPU rather than CPU.
use_gpu_for_driver = True
num_workers_per_device_docstring = Number of workers to place on a single device (CPU or GPU).
num_workers_per_device = 1

# Optional amount of memory (in bytes) to start the ray object store with.
object_store_memory=2e9
# Optional max amount of memory (in bytes) to allow each redis shard to use.
redis_max_memory=

# Gradients are clipped by this amount per update. Default is 40.
grad_clip = 40

[parameters_cleanup]
entropy_coeff = -.00176
lr_init = 0.00126
lr_final = 0.000012
# Add arbitrary amount of entropy tuning parameters. Used for tune.grid_search()
entropy_tune = [0, -1e-1]

[parameters_harvest]
entropy_coeff = -.000687
lr_init = 0.00136
lr_final = 0.000028
# Add arbitrary amount of entropy tuning parameters. Used for tune.grid_search()
entropy_tune = [0, -1e-1, -1e-2]

# Hyperparameters for the Model of Other Agents (MOA) experiment
[parameters_cleanup_a3c_baseline]
entropy_coeff = -.00176
lr_init = .00126
lr_final = .000012
[parameters_cleanup_moa_baseline]
entropy_coeff = -.00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = 0
moa_loss_weight = 1.312
train_moa_only_when_visible = False
[parameters_cleanup_influence_moa]
entropy_coeff = -.00176
lr_init = .00123
lr_final = .000012
influence_weight_beta = .620
moa_loss_weight = 15.007
curriculum_c = 40
policy_comparison = kl
train_moa_only_when_visible = True
[parameters_harvest_a3c_baseline]
entropy_coeff = -.000687
lr_init = .00136
lr_final = .000028
[parameters_harvest_moa_baseline]
entropy_coeff = -.00495
lr_init = .00206
lr_final = .000022
influence_weight_beta = 0
moa_loss_weight = 1.711
train_moa_only_when_visible = False
[parameters_harvest_influence_moa]
entropy_coeff = -.00223
lr_init = .00120
lr_final = .000044
influence_weight_beta = 2.521
moa_loss_weight = 10.911
curriculum_c = 226
policy_comparison = kl
train_moa_only_when_visible = True
[parameters_switch_a3c_baseline]
entropy_coeff = -.000687
lr_init = .00136
lr_final = .000028
entropy_tune = [-0.001, -0.01, -0.1]
[parameters_switch_curiosity]
entropy_coeff = -.001
lr_init = .001
lr_final = .0001
aux_loss_weight = 1
aux_reward_weight = 0.01
num_switches = 6
entropy_tune = [-0.001, -0.01, -0.1]
aux_loss_weight_tune = [.1, 1, 10]
aux_reward_weight_tune = [.1, 1, 10]

[network]
# Optional Redis address. If this address is not provided, a local Redis server is automatically started.
# Leave empty if not used.
redis_address =
# Optional server address to sync training results to.
# Leave empty if not used.
upload_dir =

[paths]
# Path where ray_results are stored
ray_results_path = ~/ray_results
# Path where plotted ray_results are stored
plot_path = ~/ray_results_plotted
# Path where videos are stored
video_path = ~/ray_results_videos
